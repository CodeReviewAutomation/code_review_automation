{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"FineTuning.ipynb","provenance":[{"file_id":"https://github.com/masies/CRA/blob/main/replication_package/Replication_package_FineTuning.ipynb","timestamp":1629903274244}],"collapsed_sections":["oDvropGqwja9","l8zPvNrpypq7","32bONo87yxhA","-wrG3YHXy2qp","bVPMF378zg0X","quC7CexKzjOM"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"qWgILLAAy_N-"},"source":["# T5 Fine_Tuning\n","\n","in this notebook we will fine-tune different models on the datasets we already processed.\n","\n","## NOTEBOOK SETTINGS\n","\n","We recommend to use \"high ram\" setting for this notebook\n","you can changed this in the colab menu : `Runtime > Change runtime type`\n","\n","\n","We start by setting the environment connecting colab to the Google Cloud Storage (GCS) bucket and setting everything up for the TPU processor. (This colab uses TPU and high ram settings)"]},{"cell_type":"code","metadata":{"id":"FX9_YlEdy8gz","executionInfo":{"status":"ok","timestamp":1630591290328,"user_tz":-120,"elapsed":3,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["from google.colab import auth\n","auth.authenticate_user()\n","#@title ## Set Your GCS credential\n","project_id = '' #@param {type:\"string\"}\n","bucket_name = '' #@param {type:\"string\"}\n","\n","!gcloud config set project {project_id}\n","\n","!pip3 install --upgrade pip\n","!pip3 install t5==0.9.0\n","!pip3 install tensorflow==2.6.0\n","!pip3 install keras==2.6.0\n","!pip3 install gin-config\n","\n","import functools\n","import os\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","\n","import tensorflow.compat.v1 as tf\n","import tensorflow_datasets as tfds\n","\n","import t5\n","\n","#Set the base dir(Google cloud bucket)\n","BASE_DIR = \"gs://\" + bucket_name \n","\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BASE_DIR.\")\n","ON_CLOUD = True\n","\n","\n","if ON_CLOUD:\n","  import tensorflow_gcs_config\n","  from google.colab import auth\n","  # Set credentials for GCS reading/writing from Colab and TPU.\n","  TPU_TOPOLOGY = \"2x2\"\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    TPU_ADDRESS = tpu.get_master()\n","    print('Running on TPU:', TPU_ADDRESS)\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","  auth.authenticate_user()\n","  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n","\n","tf.disable_v2_behavior()\n","\n","# Improve logging.\n","from contextlib import contextmanager\n","import logging as py_logging\n","\n","if ON_CLOUD:\n","  tf.get_logger().propagate = False\n","  py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZAo86QxiwX9F"},"source":["We specify the paths and the sizes of all our datasets to later build our tasks."]},{"cell_type":"code","metadata":{"id":"IC03zV3sy8oT","executionInfo":{"status":"ok","timestamp":1630591310015,"user_tz":-120,"elapsed":208,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["## tasks large dataset\n","nq_tsv_path_code_code_large = {\n","    \"train\":      'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/new_large/code-to-code/train.tsv',\n","    \"validation\": 'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/new_large/code-to-code/val.tsv'\n","}\n","\n","!gsutil cp {nq_tsv_path_code_code_large[\"train\"]} ./train.tsv\n","!gsutil cp {nq_tsv_path_code_code_large[\"validation\"]} ./val.tsv\n","\n","data_train = len([line for line in open('./train.tsv', 'r')])\n","data_val = len([line for line in open('./val.tsv', 'r')])\n","\n","num_nq_examples_code_code_large = dict(train=data_train, validation=data_val)\n","\n","nq_tsv_path_code_comment_large = {\n","    \"train\":      'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/new_large/code-to-comment/train.tsv',\n","    \"validation\": 'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/new_large/code-to-comment/val.tsv'\n","}\n","\n","!gsutil cp {nq_tsv_path_code_comment_large[\"train\"]} ./train.tsv\n","!gsutil cp {nq_tsv_path_code_comment_large[\"validation\"]} ./val.tsv\n","\n","data_train = len([line for line in open('./train.tsv', 'r')])\n","data_val = len([line for line in open('./val.tsv', 'r')])\n","\n","num_nq_examples_code_comment_large = dict(train=data_train, validation=data_val)\n","\n","nq_tsv_path_codeANDcomment_code_large = {\n","    \"train\":      'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/new_large/code&comment-to-code/train.tsv',\n","    \"validation\": 'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/new_large/code&comment-to-code/val.tsv'\n","}\n","\n","!gsutil cp {nq_tsv_path_codeANDcomment_code_large[\"train\"]} ./train.tsv\n","!gsutil cp {nq_tsv_path_codeANDcomment_code_large[\"validation\"]} ./val.tsv\n","\n","data_train = len([line for line in open('./train.tsv', 'r')])\n","data_val = len([line for line in open('./val.tsv', 'r')])\n","\n","num_nq_examples_codeANDcomment_code_large = dict(train=data_train, validation=data_val)\n","\n","## tasks small dataset\n","nq_tsv_path_code_code_small = {\n","    \"train\":      'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/Tufano_etal_ICSE21/code-to-code/train.tsv',\n","    \"validation\": 'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/Tufano_etal_ICSE21/code-to-code/val.tsv'\n","}\n","\n","!gsutil cp {nq_tsv_path_code_code_small[\"train\"]} ./train.tsv\n","!gsutil cp {nq_tsv_path_code_code_small[\"validation\"]} ./val.tsv\n","\n","data_train = len([line for line in open('./train.tsv', 'r')])\n","data_val = len([line for line in open('./val.tsv', 'r')])\n","\n","num_nq_examples_code_code_small = dict(train=data_train, validation=data_val)\n","\n","nq_tsv_path_codeANDcomment_code_small = {\n","    \"train\":      'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/Tufano_etal_ICSE21/code&comment-to-code/train.tsv',\n","    \"validation\": 'gs://' + bucket_name + '/automating_code_review/dataset/fine-tuning/Tufano_etal_ICSE21/code&comment-to-code/val.tsv'\n","}\n","\n","!gsutil cp {nq_tsv_path_codeANDcomment_code_small[\"train\"]} ./train.tsv\n","!gsutil cp {nq_tsv_path_codeANDcomment_code_small[\"validation\"]} ./val.tsv\n","\n","data_train = len([line for line in open('./train.tsv', 'r')])\n","data_val = len([line for line in open('./val.tsv', 'r')])\n","\n","num_nq_examples_codeANDcomment_code_small = dict(train=data_train, validation=data_val)\n","\n","!rm ./train.tsv\n","!rm ./val.tsv"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"QBVqrfNF4EoQ"},"source":["We specify the model and vocab path of the previusly trained sentencepiece tokenizer model in the GCS bucket"]},{"cell_type":"code","metadata":{"id":"_twenV5ZwhPf"},"source":["from t5.data import postprocessors as t5_postprocessors\n","from t5.seqio import Feature,SentencePieceVocabulary\n","\n","vocab_model_path = 'gs://' + bucket_name + '/automating_code_review/tokenizer/TokenizerModel.model'\n","vocab_path = 'gs://' + bucket_name + '/automating_code_review/tokenizer/TokenizerModel.vocab'\n","\n","TaskRegistry = t5.data.TaskRegistry\n","TfdsTask = t5.data.TfdsTask\n","\n","def get_default_vocabulary():\n","  return SentencePieceVocabulary(vocab_model_path, 100)\n","\n","DEFAULT_OUTPUT_FEATURES = {\n","    \"inputs\": Feature(\n","        vocabulary=get_default_vocabulary(), add_eos=True, required=False),\n","\n","    \"targets\": Feature(\n","        vocabulary=get_default_vocabulary(), add_eos=True)\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oDvropGqwja9"},"source":["# Setting up all the tasks\n","\n","We will set the following tasks\n","- code-to-code (new large dataset)\n","- code-to-code (Tufano etal. dataset)\n","- code&comment-to-code (new large dataset)\n","- code&comment-to-code (Tufano etal. dataset)\n","- code-to-comment (new large dataset)\n","\n","then we will later chose which one or which mixture to tune\n"]},{"cell_type":"markdown","metadata":{"id":"l8zPvNrpypq7"},"source":["## TASK : CODE to CODE on new large dataset\n","- task name = `code-to-code_new_large`\n","- task prefix = `code2code: `"]},{"cell_type":"code","metadata":{"id":"ddHRgYezy8uE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630532698130,"user_tz":-120,"elapsed":3983,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}},"outputId":"d98b46e7-8dd5-4b93-b64a-caaebebffbbd"},"source":["def nq_dataset_code_code_large(split, shuffle_files=True):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(nq_tsv_path_code_code_large[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","print(\"A few raw validation examples...\")\n","for ex in tfds.as_numpy(nq_dataset_code_code_large(\"validation\").take(2)):\n","  print(ex)\n","print(\"A few raw training examples...\")\n","for ex in tfds.as_numpy(nq_dataset_code_code_large(\"train\").take(2)):\n","  print(ex)\n","\n","def code_code_preprocessing(ds):\n","  def to_inputs_and_targets(ex):\n","        inputs = tf.strings.join(['code2code: ' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","    \n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","t5.data.TaskRegistry.remove('code_to_code_new_large')\n","t5.data.TaskRegistry.add(\n","    \"code_to_code_new_large\",\n","    dataset_fn=nq_dataset_code_code_large,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[code_code_preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    num_input_examples=num_nq_examples_code_code_large\n",")\n","\n","nq_task = t5.data.TaskRegistry.get(\"code_to_code_new_large\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","print(\"A few preprocessed training examples...\")\n","for ex in tfds.as_numpy(ds.take(3)):\n","  print(ex)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["A few raw validation examples...\n","{'input': b'public static int positionToDragCursor(int swtPositionConstant) { switch (swtPositionConstant) { case SWT.LEFT: return LEFT; case SWT.RIGHT: return RIGHT; case SWT.TOP: return TOP; case SWT.BOTTOM: return BOTTOM; case SWT.CENTER: return CENTER; } return INVALID; }', 'output': b'public static int positionToDragCursor(int swtPositionConstant) { switch (swtPositionConstant) { case SWT.LEFT: return LEFT; case SWT.RIGHT: return RIGHT; case SWT.TOP: return TOP; case SWT.BOTTOM: return BOTTOM; case SWT.CENTER: return CENTER; default: return INVALID; } }'}\n","{'input': b'public static boolean shouldCaptureIncrementalChanges(FileSystem fs,Path rootDir) throws StandardException{ boolean shouldRegister = false; try { boolean enabled = incrementalBackupEnabled(); if (enabled) { RecoverableZooKeeper zooKeeper = ZkUtils.getRecoverableZooKeeper(); String spliceBackupPath = BackupUtils.getBackupPath(); if (zooKeeper.exists(spliceBackupPath, false)==null){ return false; } boolean isRestoreMode = SIDriver.driver().lifecycleManager().isRestoreMode(); if (!isRestoreMode) { if (BackupUtils.existsDatabaseBackup(fs, rootDir)) { if (LOG.isDebugEnabled()) { SpliceLogUtils.debug(LOG, \"There exists a successful full or incremental backup in the system\"); } shouldRegister = true; } else { List<String> backupJobs = zooKeeper.getChildren(spliceBackupPath, false); for (String backupId : backupJobs) { String path = spliceBackupPath + \"/\" + backupId; byte[] data = zooKeeper.getData(path, false, null); BackupJobStatus status = BackupJobStatus.parseFrom(data); if (status.getScope() == BackupJobStatus.BackupScope.DATABASE.DATABASE) { if (LOG.isDebugEnabled()) { SpliceLogUtils.debug(LOG, \"A database backup is running\"); } shouldRegister = true; } } } } } return shouldRegister; } catch (Exception e) { e.printStackTrace(); throw StandardException.plainWrapException(e); } }', 'output': b'public static boolean shouldCaptureIncrementalChanges(FileSystem fs,Path rootDir) throws StandardException{ boolean shouldRegister = false; try { boolean enabled = incrementalBackupEnabled(); if (enabled) { RecoverableZooKeeper zooKeeper = ZkUtils.getRecoverableZooKeeper(); String spliceBackupPath = BackupUtils.getBackupPath(); if (zooKeeper.exists(spliceBackupPath, false)==null){ return false; } boolean isRestoreMode = SIDriver.driver().lifecycleManager().isRestoreMode(); if (!isRestoreMode) { if (BackupUtils.existsDatabaseBackup(fs, rootDir)) { if (LOG.isDebugEnabled()) { SpliceLogUtils.debug(LOG, \"There exists a successful full or incremental backup in the system\"); } shouldRegister = true; } else { List<String> backupJobs = zooKeeper.getChildren(spliceBackupPath, false); for (String backupId : backupJobs) { String path = spliceBackupPath + \"/\" + backupId; byte[] data = zooKeeper.getData(path, false, null); BackupJobStatus status = BackupJobStatus.parseFrom(data); if (status.getScope() == BackupJobStatus.BackupScope.DATABASE) { if (LOG.isDebugEnabled()) { SpliceLogUtils.debug(LOG, \"A database backup is running\"); } shouldRegister = true; } } } } } return shouldRegister; } catch (Exception e) { e.printStackTrace(); throw StandardException.plainWrapException(e); } }'}\n","A few raw training examples...\n","{'input': b'public void testMissingPrimaryKeyLookupQuery() throws Exception { indexType2TweeterDocuments(); final Table table = dataContext.getDefaultSchema().getTableByName(DEFAULT_TABLE_NAME); final Column[] pks = table.getPrimaryKeys().toArray(new Column[0]); try (DataSet ds = dataContext.query().from(table).selectAll().where(pks[0]).eq(\"missing\").execute()) { assertFalse(ds.next()); } }', 'output': b'public void testMissingPrimaryKeyLookupQuery() throws Exception { indexType2TweeterDocuments(); final Table table = dataContext.getDefaultSchema().getTableByName(DEFAULT_TABLE_NAME); final Column[] primaryKeys = table.getPrimaryKeys().toArray(new Column[0]); try (final DataSet dataSet = dataContext .query() .from(table) .selectAll() .where(primaryKeys[0]) .eq(\"missing\") .execute()) { assertFalse(dataSet.next()); } }'}\n","{'input': b'public void loadJavaRDD() throws Exception { doReturn(javaPairRDD).when(sparkContext).newAPIHadoopFile(any(), any(), any(), any(), any()); doReturn(javaRDD).when(javaPairRDD).map(any()); JavaRDD<Country> retJavaRDD = SparkAvroLoader.loadJavaRDD(sparkContext, \"/avro/datastore\", Country.class); assertTrue(javaRDD == retJavaRDD); verify(sparkContext).newAPIHadoopFile( eq(\"/avro/datastore\"), eq(AvroKeyInputFormat.class), eq(Country.class), eq(NullWritable.class), configurationCaptor.capture()); assertEquals(Country.SCHEMA$.toString(), configurationCaptor.getValue().get(\"avro.schema.input.key\")); verify(javaPairRDD).map(mapFunctionCaptor.capture()); assertMapFunction(mapFunctionCaptor.getValue()); verifyNoMoreInteractions(sparkContext, javaPairRDD); verifyZeroInteractions(javaRDD); }', 'output': b'public void loadJavaRDD() throws Exception { doReturn(javaPairRDD).when(sparkContext).newAPIHadoopFile(eq(\"/avro/datastore\"), eq(AvroKeyInputFormat.class), eq(Country.class), eq(NullWritable.class), any(Configuration.class)); doReturn(javaRDD).when(javaPairRDD).map(any(Function.class)); JavaRDD<Country> retJavaRDD = SparkAvroLoader.loadJavaRDD(sparkContext, \"/avro/datastore\", Country.class); assertTrue(javaRDD == retJavaRDD); verify(sparkContext).newAPIHadoopFile( eq(\"/avro/datastore\"), eq(AvroKeyInputFormat.class), eq(Country.class), eq(NullWritable.class), configurationCaptor.capture()); assertEquals(Country.SCHEMA$.toString(), configurationCaptor.getValue().get(\"avro.schema.input.key\")); verify(javaPairRDD).map(mapFunctionCaptor.capture()); assertMapFunction(mapFunctionCaptor.getValue()); verifyNoMoreInteractions(sparkContext, javaPairRDD); verifyZeroInteractions(javaRDD); }'}\n","A few preprocessed training examples...\n","{'inputs_pretokenized': b'code2code: SuggestReviewers(AccountVisibility av, AccountInfo.Loader.Factory accountLoaderFactory, AccountControl.Factory accountControlFactory, AccountCache accountCache, GroupMembers.Factory groupMembersFactory, IdentifiedUser.GenericFactory identifiedUserFactory, Provider<CurrentUser> currentUser, Provider<ReviewDb> dbProvider, @GerritServerConfig Config cfg, GroupBackend groupBackend) { this.accountLoaderFactory = accountLoaderFactory; this.accountControlFactory = accountControlFactory; this.accountCache = accountCache; this.groupMembersFactory = groupMembersFactory; this.dbProvider = dbProvider; this.identifiedUserFactory = identifiedUserFactory; this.currentUser = currentUser; this.groupBackend = groupBackend; this.cfg = cfg; String suggest = cfg.getString(\"suggest\", null, \"accounts\"); if (\"OFF\".equalsIgnoreCase(suggest) || \"false\".equalsIgnoreCase(suggest)) { this.suggestAccounts = false; } else { this.useFullTextSearch = cfg.getBoolean(\"suggest\", \"fulltextsearch\", false); this.suggestAccounts = (av != AccountVisibility.NONE); } this.suggestFrom = cfg.getInt(\"suggest\", null, \"from\", 0); this.maxAllowed = cfg.getInt(\"addreviewer\", \"maxAllowed\", PostReviewers.DEFAULT_MAX_REVIEWERS); }', 'inputs': array([   91,   114,   221,    22,     9, 20239,  8229,  3252,    11,\n","        2251,  6388,    19,   387,    13,  4999,   372,    10,  2601,\n","          10,   364,  1270,  2601,   364,    13,  4999,  2183,    10,\n","         364,  1270,  2183,   364,    13,  4999,   854,  1270,   854,\n","          13,     9, 21571,    21,    10,   364,   634,  6565,   364,\n","          13,     9, 16718,  5549,   643,    10,  4068,   364,  6428,\n","         643,   364,    13,     9,   790,    42, 28443,    34,     9,\n","       17688,    13,     9,   790,    42,  8229,  3368,    34,  1424,\n","         790,    13,    29,   967,  2143,   915, 25003,     9,   438,\n","       10975,    13,     9,   640,  9710,   634,  9710,    14,    17,\n","          38,    10,  4432,  2601,   364,    18,  1270,  2601,   364,\n","          26,    38,    10,  4432,  2183,   364,    18,  1270,  2183,\n","         364,    26,    38,    10,  4432,   854,    18,  1270,   854,\n","          26,    38,    10,   973,  6565,   364,    18,   634,  6565,\n","         364,    26,    38,    10,  1459,   790,    18,  1424,   790,\n","          26,    38,    10,  9436,  5549,   643,   364,    18,  6428,\n","         643,   364,    26,    38,    10, 17688,    18,     9, 17688,\n","          26,    38,    10,   973,  9710,    18,   634,  9710,    26,\n","          38,    10,  7109,    18, 10975,    26,    55,  1169,    18,\n","       10975,    10,  1008,    48, 25530,    88,    51,    13,    43,\n","       15896,    80,    30,     9,    48,  8306,    56,    10,  1940,\n","          11, 25530,    14,   344,    43,   609,    56,    10,  1940,\n","          11, 25530,   197,    17,    38,    10, 25530, 13327,    18,\n","         209,    26,    15,   123,    17,    38,    10,  1909, 27510,\n","        2021,    18, 10975,    10,  6707,    48, 25530,    88,    43,\n","        4980,   502,  2006,    88,   209,    20,    38,    10, 25530,\n","       13327,    18,    25,  6627,   127,  4999,  6388,    10,  3874,\n","          20,    15,    38,    10, 25530,   692,    18, 10975,    10,\n","        2841,    48, 25530,    88,    51,    13,    43,   927,    88,\n","        1736,    38,    10,   925,  4495,    18, 10975,    10,  2841,\n","          48,   119,   167, 15897,    88,    43,   925,  4495,    88,\n","        6008,  8229,  3252,    10,  2032,    27,  1771,    27, 30745,\n","       20762,    20,    15,     1], dtype=int32), 'targets_pretokenized': b'SuggestReviewers(AccountVisibility av, AccountInfo.Loader.Factory accountLoaderFactory, AccountControl.Factory accountControlFactory, AccountCache accountCache, GroupMembers.Factory groupMembersFactory, IdentifiedUser.GenericFactory identifiedUserFactory, Provider<CurrentUser> currentUser, Provider<ReviewDb> dbProvider, @GerritServerConfig Config cfg, GroupBackend groupBackend) { this.accountLoaderFactory = accountLoaderFactory; this.accountControlFactory = accountControlFactory; this.accountCache = accountCache; this.groupMembersFactory = groupMembersFactory; this.dbProvider = dbProvider; this.identifiedUserFactory = identifiedUserFactory; this.currentUser = currentUser; this.groupBackend = groupBackend; this.maxSuggestedReviewers = cfg.getInt(\"suggest\", \"maxsuggestedreviewers\", DEFAULT_MAX_SUGGESTED); this.fullTextMaxMatches = cfg.getInt(\"suggest\", \"fulltextsearchmaxmatches\", DEFAULT_MAX_MATCHES); String suggest = cfg.getString(\"suggest\", null, \"accounts\"); if (\"OFF\".equalsIgnoreCase(suggest) || \"false\".equalsIgnoreCase(suggest)) { this.suggestAccounts = false; } else { this.useFullTextSearch = cfg.getBoolean(\"suggest\", \"fulltextsearch\", false); this.suggestAccounts = (av != AccountVisibility.NONE); } this.suggestFrom = cfg.getInt(\"suggest\", null, \"from\", 0); this.maxAllowed = cfg.getInt(\"addreviewer\", \"maxAllowed\", PostReviewers.DEFAULT_MAX_REVIEWERS); }', 'targets': array([    9, 20239,  8229,  3252,    11,  2251,  6388,    19,   387,\n","          13,  4999,   372,    10,  2601,    10,   364,  1270,  2601,\n","         364,    13,  4999,  2183,    10,   364,  1270,  2183,   364,\n","          13,  4999,   854,  1270,   854,    13,     9, 21571,    21,\n","          10,   364,   634,  6565,   364,    13,     9, 16718,  5549,\n","         643,    10,  4068,   364,  6428,   643,   364,    13,     9,\n","         790,    42, 28443,    34,     9, 17688,    13,     9,   790,\n","          42,  8229,  3368,    34,  1424,   790,    13,    29,   967,\n","        2143,   915, 25003,     9,   438, 10975,    13,     9,   640,\n","        9710,   634,  9710,    14,    17,    38,    10,  4432,  2601,\n","         364,    18,  1270,  2601,   364,    26,    38,    10,  4432,\n","        2183,   364,    18,  1270,  2183,   364,    26,    38,    10,\n","        4432,   854,    18,  1270,   854,    26,    38,    10,   973,\n","        6565,   364,    18,   634,  6565,   364,    26,    38,    10,\n","        1459,   790,    18,  1424,   790,    26,    38,    10,  9436,\n","        5549,   643,   364,    18,  6428,   643,   364,    26,    38,\n","          10, 17688,    18,     9, 17688,    26,    38,    10,   973,\n","        9710,    18,   634,  9710,    26,    38,    10,   925, 21024,\n","        8229,  3252,    18, 10975,    10,  2841,    48, 25530,    88,\n","          43,   925, 25530,   246,   167, 15897,    21,    88,  4552,\n","          27,  1771,    27, 15234, 16065, 10109,  1798,    20,    38,\n","          10,  4980,   595,  2013,  5345,    18, 10975,    10,  2841,\n","          48, 25530,    88,    43,  4980,   502,  2006,   925,  1934,\n","          88,  4552,    27,  1771,    27,  6290,  5387,    20,    55,\n","        1169,    18, 10975,    10,  1008,    48, 25530,    88,    51,\n","          13,    43, 15896,    80,    30,     9,    48,  8306,    56,\n","          10,  1940,    11, 25530,    14,   344,    43,   609,    56,\n","          10,  1940,    11, 25530,   197,    17,    38,    10, 25530,\n","       13327,    18,   209,    26,    15,   123,    17,    38,    10,\n","        1909, 27510,  2021,    18, 10975,    10,  6707,    48, 25530,\n","          88,    43,  4980,   502,  2006,    88,   209,    20,    38,\n","          10, 25530, 13327,    18,    25,  6627,   127,  4999,  6388,\n","          10,  3874,    20,    15,    38,    10, 25530,   692,    18,\n","       10975,    10,  2841,    48, 25530,    88,    51,    13,    43,\n","         927,    88,  1736,    38,    10,   925,  4495,    18, 10975,\n","          10,  2841,    48,   119,   167, 15897,    88,    43,   925,\n","        4495,    88,  6008,  8229,  3252,    10,  2032,    27,  1771,\n","          27, 30745, 20762,    20,    15,     1], dtype=int32)}\n","{'inputs_pretokenized': b'code2code: public User addUserToProject(final Project project, final Account account, final Role... userRoles) { notNull(project, \"project\"); notNull(account, \"account\"); notEmpty(account.getUri(), \"account.uri\"); notNull(userRoles, \"userRoles\"); validateRoleURIs(userRoles); noNullElements(Arrays.stream(userRoles).map(Role::getUri).collect(Collectors.toList()), \"userRoles.uri\"); notEmpty(project.getId(), \"project.id\"); final User user = new User(account, userRoles); doPostProjectUsersUpdate(project, user); return getUser(project, account); }', 'inputs': array([   91,   114,   221,    22,    46,  1197,   182,   643,   315,\n","        1784,    11,   232,  2138,   382,    13,    92,  4999,  1270,\n","          13,    92, 11461,   418,   266,  7008,    14,    17,     9,\n","        3782,    11,  1325,    13,    43,  1325,    80,     9,  3782,\n","          11,  4432,    13,    43,  4432,    80,     9, 17590,    11,\n","        4432,    10, 11647,   147,    43,  4432,    10,  2167,    80,\n","           9,  3782,    11,   430,  7008,    13,    43,   430,  7008,\n","          80,  1834,  2649,  1292,    21,    11,   430,  7008,    20,\n","         198,  3175,  2106,    11,  1539,    10,   591,    11,   430,\n","        7008,   149,   516,    11,  2649,   950, 11647,   149,  1324,\n","          11,  1765,    10,  2556,   103,    13,    43,   430,  7008,\n","          10,  2167,    80,     9, 17590,    11,  1325,    10,  1266,\n","         147,    43,  1325,    10,   165,    80,    92,  1197,   266,\n","          18,    35,  1197,    11,  4432,    13,   266,  7008,    20,\n","       12863,  1784,  2286,  1296,    11,  1325,    13,   266,    20,\n","          44, 10255,    11,  1325,    13,  1270,    20,    15,     1],\n","      dtype=int32), 'targets_pretokenized': b'public User addUserToProject(final Project project, final Account account, final Role... userRoles) { notNull(project, \"project\"); notNull(account, \"account\"); notEmpty(account.getUri(), \"account.uri\"); notNull(userRoles, \"userRoles\"); validateRoleURIs(userRoles); notEmpty(project.getId(), \"project.id\"); final User user = new User(account, userRoles); doPostProjectUsersUpdate(project, user); return getUser(project, account); }', 'targets': array([   46,  1197,   182,   643,   315,  1784,    11,   232,  2138,\n","         382,    13,    92,  4999,  1270,    13,    92, 11461,   418,\n","         266,  7008,    14,    17,     9,  3782,    11,  1325,    13,\n","          43,  1325,    80,     9,  3782,    11,  4432,    13,    43,\n","        4432,    80,     9, 17590,    11,  4432,    10, 11647,   147,\n","          43,  4432,    10,  2167,    80,     9,  3782,    11,   430,\n","        7008,    13,    43,   430,  7008,    80,  1834,  2649,  1292,\n","          21,    11,   430,  7008,    20,     9, 17590,    11,  1325,\n","          10,  1266,   147,    43,  1325,    10,   165,    80,    92,\n","        1197,   266,    18,    35,  1197,    11,  4432,    13,   266,\n","        7008,    20, 12863,  1784,  2286,  1296,    11,  1325,    13,\n","         266,    20,    44, 10255,    11,  1325,    13,  1270,    20,\n","          15,     1], dtype=int32)}\n","{'inputs_pretokenized': b'code2code: private void load(File directory) throws ConfigurationException { if (!directory.exists()) { log.warn(\"The directory \\\\\"\" + directory.getAbsolutePath() + \"\\\\\" containing the configuration files doesn\\'t \" + \"exist.\"); } else { File[] files = directory.listFiles(); if (files != null) { sort(files); for (File file : files) { if (file.getName().endsWith(\".properties\")) { try { ExtensionEntry entry = new ExtensionEntry(file); ExtensionEntry alreadyLoded = loadedEntries.get(entry.getName()); if (alreadyLoded != null) { throw new ConfigurationException(\"Could not load the configuration file \\\\\"\" + file.getAbsolutePath() + \"\\\\\". The configuration file + \\\\\"\" + alreadyLoded.file.getAbsolutePath() + \"\\\\\" already has the name \" + entry.getName()); } loadedEntries.put(entry.name, entry); } catch (IOException exception) { throw new ConfigurationException( \"Can\\'t load object configuration file \\\\\"\" + file.getAbsolutePath() + \"\\\\\".\", exception); } } } } } }', 'inputs': array([   91,   114,   221,    22,   126,    73,   771,    11,   255,\n","         571,    14,   171,     9,   425,    68,    17,    30,   389,\n","        2916,    10,  2314,   103,    17,   453,    10,  2008,    48,\n","        1043,   571,  8885,    63,   571,    10,  3442,    45,    63,\n","        4917,  1106,    12,   492,   429,   370,    31,    59,    43,\n","          63,    43, 18067,    10,    80,    15,   123,    17,   378,\n","         102,   429,    18,   571,    10,  5466,    41,    30,    25,\n","        3339,   127,    51,    14,    17,  1224,    11,  3339,    20,\n","          36,    25,   255,   111,     9,    22,   429,    14,    17,\n","          30,    25,   379,    10,   433,    83,  3233,    48,    10,\n","         948,   729,    17,   125,    17, 13365,   460,   529,    18,\n","          35, 13365,   460,    11,   379,    20, 13365,   460,   461,\n","       10884, 14717,    18,  1585,  2925,    10,    64,    11,   936,\n","          10,   433,    95,    30,    25, 15358, 10884, 14717,   127,\n","          51,    14,    17,   163,    35,     9,   425,    68,    48,\n","        3786,    58,   771,    12,   492,   111,  8885,    63,   111,\n","          10,  3442,    45,    63,  4917,    10,    61,   492,   111,\n","          63,  8885,    63,   461, 10884, 14717,    10,   379,    10,\n","        3442,    45,    63,  4917,   461,   161,    12,    99,    43,\n","          63,   529,    10,   433,    95,    15,  1585,  2925,    10,\n","         263,    11,   936,    10,   157,    13,   529,    20,    15,\n","         174,    25,   782,   358,    14,    17,   163,    35,     9,\n","         425,    68,    11,    43,  4227,    31,    59,   771,   118,\n","         492,   111,  8885,    63,   111,    10,  3442,    45,    63,\n","        4917,    10,    88,   358,    20,    15,    15,    15,    15,\n","          15,    15,     1], dtype=int32), 'targets_pretokenized': b'private void load(File directory) throws ConfigurationException { if (!directory.exists()) { log.warn(\"The directory \\\\\"\" + directory.getAbsolutePath() + \"\\\\\" containing the configuration files doesn\\'t \" + \"exist.\"); } else { File[] files = directory.listFiles(); if (files != null) { sort(files); for (File file : files) { if (file.getName().endsWith(\".properties\")) { try { ExtensionEntry entry = new ExtensionEntry(file); ExtensionEntry alreadyLoded = loadedEntries.get(entry.getName()); if (alreadyLoded != null) { throw new ConfigurationException(String.format(\"Could not load the configuration file \\\\\"%1$s\\\\\". The configuration file + \\\\\"%2$s\\\\\" already has the name %3$s\", file.getAbsolutePath(), alreadyLoded.file.getAbsolutePath(), entry.getName())); } loadedEntries.put(entry.name, entry); } catch (IOException exception) { throw new ConfigurationException(String.format(\"Can\\'t load object configuration file \\\\\"%1$s\\\\\"\", file.getAbsolutePath())); } } } } } }', 'targets': array([  126,    73,   771,    11,   255,   571,    14,   171,     9,\n","         425,    68,    17,    30,   389,  2916,    10,  2314,   103,\n","          17,   453,    10,  2008,    48,  1043,   571,  8885,    63,\n","         571,    10,  3442,    45,    63,  4917,  1106,    12,   492,\n","         429,   370,    31,    59,    43,    63,    43, 18067,    10,\n","          80,    15,   123,    17,   378,   102,   429,    18,   571,\n","          10,  5466,    41,    30,    25,  3339,   127,    51,    14,\n","          17,  1224,    11,  3339,    20,    36,    25,   255,   111,\n","           9,    22,   429,    14,    17,    30,    25,   379,    10,\n","         433,    83,  3233,    48,    10,   948,   729,    17,   125,\n","          17, 13365,   460,   529,    18,    35, 13365,   460,    11,\n","         379,    20, 13365,   460,   461, 10884, 14717,    18,  1585,\n","        2925,    10,    64,    11,   936,    10,   433,    95,    30,\n","          25, 15358, 10884, 14717,   127,    51,    14,    17,   163,\n","          35,     9,   425,    68,    11,    52,    10,   581,    48,\n","        3786,    58,   771,    12,   492,   111,     9, 19327,   133,\n","           2,    21,  2094,    10,    61,   492,   111,    63,  3014,\n","        6156,     2,    21,  2094,   461,   161,    12,    99,     9,\n","       11827,     2,    21,    88,   111,    10,  3442,   147,   461,\n","       10884, 14717,    10,   379,    10,  3442,   147,   529,    10,\n","         433,   880,    15,  1585,  2925,    10,   263,    11,   936,\n","          10,   157,    13,   529,    20,    15,   174,    25,   782,\n","         358,    14,    17,   163,    35,     9,   425,    68,    11,\n","          52,    10,   581,    48,  4227,    31,    59,   771,   118,\n","         492,   111,     9, 19327,   133,     2,    21, 16325,   111,\n","          10,  3442,   880,    15,    15,    15,    15,    15,    15,\n","           1], dtype=int32)}\n"]}]},{"cell_type":"markdown","metadata":{"id":"32bONo87yxhA"},"source":["## TASK : CODE to COMMENT on new large dataset\n","- task name = `code-to-comment_new_large`\n","- task prefix = `code2comment: `"]},{"cell_type":"code","metadata":{"id":"p68qZUBTxWiF"},"source":["def nq_dataset_code_comment_large(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(nq_tsv_path_code_comment_large[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","# print(\"A few raw validation examples...\")\n","# for ex in tfds.as_numpy(nq_dataset_code_comment_large(\"validation\").take(2)):\n","#   print(ex)\n","# print(\"A few raw training examples...\")\n","# for ex in tfds.as_numpy(nq_dataset_code_comment_large(\"train\").take(2)):\n","#   print(ex)\n","\n","def code_comment_preprocessing(ds):\n","  def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['code2comment: ' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","    \n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","#Create a new training task\n","t5.data.TaskRegistry.remove('code_to_comment_new_large')\n","t5.data.TaskRegistry.add(\n","    \"code_to_comment_new_large\",\n","    dataset_fn=nq_dataset_code_comment_large,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[code_comment_preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    num_input_examples=num_nq_examples_code_comment_large\n",")\n","\n","nq_task = t5.data.TaskRegistry.get(\"code_to_comment_new_large\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","# print(\"A few preprocessed training examples...\")\n","# for ex in tfds.as_numpy(ds.take(3)):\n","#   print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-wrG3YHXy2qp"},"source":["## TASK : CODE and COMMENT to CODE on new large dataset\n","- task name = `code&comment-to-code_new_large`\n","- task prefix = `code&comment2code: `"]},{"cell_type":"code","metadata":{"id":"c_kJneaJxWmm"},"source":["############### THIRD TASK : CODE&COMMENT2CODE ###############\n","\n","def nq_dataset_codeANDcomment_code_large(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(nq_tsv_path_codeANDcomment_code_large[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","# print(\"A few raw validation examples...\")\n","# for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_large(\"validation\").take(2)):\n","#   print(ex)\n","# print(\"A few raw training examples...\")\n","# for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_large(\"train\").take(2)):\n","#   print(ex)\n","\n","def codeANDcomment_code_preprocessing(ds):\n","  \n","  def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['code&comment2code: ' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","    \n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","#Create a new training task\n","t5.data.TaskRegistry.remove('code_comment_to_code_new_large')\n","t5.data.TaskRegistry.add(\n","    \"code_comment_to_code_new_large\",\n","    dataset_fn=nq_dataset_codeANDcomment_code_large,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[codeANDcomment_code_preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    num_input_examples=num_nq_examples_codeANDcomment_code_large\n",")\n","\n","nq_task = t5.data.TaskRegistry.get(\"code_comment_to_code_new_large\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","# print(\"A few preprocessed training examples...\")\n","# for ex in tfds.as_numpy(ds.take(3)):\n","#   print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bVPMF378zg0X"},"source":["## TASK : CODE to CODE on Tufano etal. ICSE21 dataset\n","- task name = `code-to-code_Tufano_etal_ICSE21`\n","- task prefix = `code2code: `"]},{"cell_type":"code","metadata":{"id":"m43eoZAKxWry"},"source":["def nq_dataset_code_code_small(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(nq_tsv_path_code_code_small[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","# print(\"A few raw validation examples...\")\n","# for ex in tfds.as_numpy(nq_dataset_code_code_small(\"validation\").take(2)):\n","#   print(ex)\n","# print(\"A few raw training examples...\")\n","# for ex in tfds.as_numpy(nq_dataset_code_code_small(\"train\").take(2)):\n","#   print(ex)\n","\n","def code_code_preprocessing(ds):\n","  \n","  def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['code2code: ' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","    \n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","#Create a new training task\n","t5.data.TaskRegistry.remove('code_to_code_Tufano_etal_ICSE21')\n","t5.data.TaskRegistry.add(\n","    \"code_to_code_Tufano_etal_ICSE21\",\n","    dataset_fn=nq_dataset_code_code_small,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[code_code_preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    num_input_examples=num_nq_examples_codeANDcomment_code_small\n",")\n","\n","nq_task = t5.data.TaskRegistry.get(\"code_to_code_Tufano_etal_ICSE21\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","# print(\"A few preprocessed training examples...\")\n","# for ex in tfds.as_numpy(ds.take(3)):\n","#   print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"quC7CexKzjOM"},"source":["## TASK : CODE and COMMENT to CODE on Tufano etal. ICSE21 dataset\n","- task name = `code&comment-to-code_Tufano_etal_ICSE21`\n","- task prefix = `code&comment2code: `"]},{"cell_type":"code","metadata":{"id":"TZU9aB3cxWuP"},"source":["def nq_dataset_codeANDcomment_code_small(split, shuffle_files=False):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(nq_tsv_path_codeANDcomment_code_small[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  \n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","# print(\"A few raw validation examples...\")\n","# for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_small(\"validation\").take(2)):\n","#   print(ex)\n","# print(\"A few raw training examples...\")\n","# for ex in tfds.as_numpy(nq_dataset_codeANDcomment_code_small(\"train\").take(2)):\n","#   print(ex)\n","\n","def marked_code_preprocessing(ds):\n","  \n","  def to_inputs_and_targets(ex):\n","\n","        inputs = tf.strings.join(['code&comment2code: ' + ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","    \n","  return ds.map(to_inputs_and_targets, \n","                num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","#Create a new training task\n","t5.data.TaskRegistry.remove('code_comment_to_code_Tufano_etal_ICSE21')\n","t5.data.TaskRegistry.add(\n","    \"code_comment_to_code_Tufano_etal_ICSE21\",\n","    dataset_fn=nq_dataset_codeANDcomment_code_small,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[marked_code_preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n","    num_input_examples=num_nq_examples_codeANDcomment_code_small\n",")\n","\n","nq_task = t5.data.TaskRegistry.get(\"code_comment_to_code_Tufano_etal_ICSE21\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","# print(\"A few preprocessed training examples...\")\n","# for ex in tfds.as_numpy(ds.take(3)):\n","#   print(ex)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n4jj7R6x4UoE"},"source":["# Setting up fine tuning tasks"]},{"cell_type":"code","metadata":{"id":"-SDyG3Z44elJ"},"source":["def _rate_num_input_examples(task):\n","  if \"train\" in task.splits:\n","    return float(task.num_input_examples(\"train\"))\n","  elif \"validation\" in task.splits:\n","    return float(task.num_input_examples(\"validation\"))\n","  else:\n","    raise ValueError(\"Task %s does not have a train or validation split.\" % (task.name))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MCvcX3uV4bZb","executionInfo":{"status":"ok","timestamp":1630591327276,"user_tz":-120,"elapsed":209,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["t5.data.MixtureRegistry.remove(\"code_to_code_new_large\")\n","t5.data.MixtureRegistry.add(\n","    \"code_to_code_new_large\",\n","    [\"code_to_code_new_large\"],\n","    default_rate=_rate_num_input_examples\n",")\n","t5.data.MixtureRegistry.remove(\"code_to_comment_new_large\")\n","t5.data.MixtureRegistry.add(\n","    \"code_to_comment_new_large\",\n","    [\"code_to_comment_new_large\"],\n","    default_rate=_rate_num_input_examples\n",")\n","\n","t5.data.MixtureRegistry.remove(\"code_comment_to_code_new_large\")\n","t5.data.MixtureRegistry.add(\n","    \"code_comment_to_code_new_large\",\n","    [\"code_comment_to_code_new_large\"],\n","    default_rate=_rate_num_input_examples\n",")\n","\n","\n","t5.data.MixtureRegistry.remove(\"code_to_code_Tufano_etal_ICSE21\")\n","t5.data.MixtureRegistry.add(\n","    \"code_to_code_Tufano_etal_ICSE21\",\n","    [\"code_to_code_Tufano_etal_ICSE21\"],\n","    default_rate=_rate_num_input_examples\n",")\n","\n","t5.data.MixtureRegistry.remove(\"code_comment_to_code_Tufano_etal_ICSE21\")\n","t5.data.MixtureRegistry.add(\n","    \"code_comment_to_code_Tufano_etal_ICSE21\",\n","    [\"code_comment_to_code_Tufano_etal_ICSE21\"],\n","    default_rate=_rate_num_input_examples\n",")"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1qt_SUnBQsbK"},"source":["Here we need to specify:\n","- if we want to fin-tuning a pre-trained model or not (and the path of the pre-trained model if needed)\n","- the dataset we want to use between the new larger dataset and the one by Tufano etal. (ICSE21)\n","- the downstream task"]},{"cell_type":"code","metadata":{"id":"diBUukTP8_4K"},"source":["# our T5 selected architecture\n","MODEL_SIZE = \"small\"\n","\n","#@title Select fine-tuning with or without pre-training\n","fine_tuning = \"fine-tuning_without_pre-training/\" #@param [\"fine-tuning_with_pre-training/\", \"fine-tuning_without_pre-training/\"]\n","\n","if fine_tuning == \"fine-tuning_without_pre-training/\":\n","  # Specify the pre-trained dir which must contain the pre-trained models, the operative_config.gin file and the checkpoint file as well\n","  PRETRAINED_DIR= 'gs://' + bucket_name + '/automating_code_review/model_dumps/pre-training/'\n","\n","#@title Select small or large dataset\n","dataset = \"new_large\" #@param [\"Tufano_etal_ICSE21\", \"new_large\"]\n","\n","#@title Selecte the task\n","if dataset == 'Tufano_etal_ICSE21':\n","  task_small = \"code-to-code/\" #@param [\"code-to-code/\",\"code&comment-to-code/\"]\n","  task = task_small\n","else:\n","  task_large = \"code-to-comment/\" #@param [\"code-to-code/\",\"code-to-comment/\",\"code&comment-to-code/\"]\n","  task = task_large\n","\n","if task == \"code-to-code/\":\n","  task_to_train = \"code_to_code_\" + dataset\n","elif task == \"code-to-comment/\":\n","  task_to_train = \"code_to_comment_\" + dataset\n","elif task == \"code&comment-to-code/\":\n","  task_to_train = \"code_comment_to_code_\" + dataset\n","\n","############ output path ############\n","MODEL_DIR = 'gs://' + bucket_name + '/automating_code_review/model_dumps/' + fine_tuning + dataset + '/' + task\n","\n","model_parallelism, train_batch_size, keep_checkpoint_max = {\n","    \"small\": (1, 128, 200),\n","    \"base\": (2, 128, 8),\n","    \"large\": (8, 64, 4),\n","    \"3B\": (8, 16, 1),\n","    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Jm72wnoR4YP0"},"source":["We set the selected learning rate scheduler"]},{"cell_type":"code","metadata":{"id":"WlsFvlRP6b_Z","executionInfo":{"status":"ok","timestamp":1630591342769,"user_tz":-120,"elapsed":214,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["from mesh_tensorflow.transformer.learning_rate_schedules import slanted_triangular \n","\n","from mesh_tensorflow.transformer.learning_rate_schedules import truncated_rsqrt\n"," \n","from tensorflow.keras.optimizers.schedules import PolynomialDecay\n","\n","starter_learning_rate = 0.05\n","end_learning_rate = 0.001\n","decay_steps = 10000\n","\n","learning_rate_fn = PolynomialDecay(\n","    starter_learning_rate,\n","    decay_steps,\n","    end_learning_rate,\n","    power=0.5)\n","\n","#@title Select a learning rate scheduler\n","learning_rate_scheduler_picker = \"constant\" #@param [\"slanted\", \"isr\", \"polynomial\", \"constant\"]\n","\n","if learning_rate_scheduler_picker == \"slanted\":\n","  selected_learning_rate_scheduler = slanted_triangular\n","  PATH_GIN_FILE = 'gs://' + bucket_name + '/automating_code_review/utils/operative_config_slanted.gin'\n","elif learning_rate_scheduler_picker == \"isr\":\n","  selected_learning_rate_scheduler = truncated_rsqrt\n","  PATH_GIN_FILE = 'gs://' + bucket_name + '/automating_code_review/utils/operative_config_isr.gin'\n","elif learning_rate_scheduler_picker == \"polynomial\":\n","  selected_learning_rate_scheduler = learning_rate_fn\n","  PATH_GIN_FILE = 'gs://' + bucket_name + '/automating_code_review/utils/operative_config_polynomial.gin'\n","elif learning_rate_scheduler_picker == \"constant\":\n","  selected_learning_rate_scheduler = 0.001\n","  PATH_GIN_FILE = 'gs://' + bucket_name + '/automating_code_review/utils/operative_config_constant.gin'\n","\n","#@title Select a learning rate scheduler\n","number_of_steps = 500 #@param {type:\"integer\"}\n","\n","tf.io.gfile.makedirs(MODEL_DIR)\n","\n","model = t5.models.MtfModel(\n","    model_dir=MODEL_DIR,\n","    tpu=TPU_ADDRESS,\n","    tpu_topology=TPU_TOPOLOGY,\n","    model_parallelism=model_parallelism,\n","    batch_size=train_batch_size,\n","    learning_rate_schedule = selected_learning_rate_scheduler,\n","    sequence_length={\"inputs\": 512, \"targets\": 512},\n","    save_checkpoints_steps=10000,\n","    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None,\n","    iterations_per_loop=100,\n",")\n","\n","!gsutil cp {PATH_GIN_FILE}  ./config.gin\n","\n","if learning_rate_scheduler_picker == \"slanted\":\n","  gin_lines = [line for line in open(\"./config.gin\")]\n","  f = open(\"./config.gin\", \"w+\")\n","  for i in range(len(gin_lines)):\n","    if i == 196 and fine_tuning == \"fine-tuning_without_pre-training/\":\n","      line = \"slanted_triangular.start_step = 0\\n\"\n","      f.write(line)\n","      continue\n","    if i == 197:\n","      line = \"slanted_triangular.total_train_steps = \" + str(number_of_steps) + '\\n'\n","      f.write(line)\n","      continue\n","    f.write(gin_lines[i])\n","  f.close()"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ylIkE_kaQO7L"},"source":["# Start Training"]},{"cell_type":"code","metadata":{"id":"BsX8mjVrBV3D","executionInfo":{"status":"ok","timestamp":1630591353403,"user_tz":-120,"elapsed":221,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["import gin\n","\n","if fine_tuning == \"fine-tuning_without_pre-training/\":\n","  # NON PRETRAINED\n","  with gin.unlock_config():    \n","      gin.parse_config_file(\"./config.gin\")\n","      TRAIN_STEPS = number_of_steps\n","      model.train(task_to_train, steps=number_of_steps)\n","\n","else:\n","  # PRETRAINED\n","  with gin.unlock_config():\n","      gin.parse_config_file(\"./config.gin\")\n","      #RUN FINE-TUNING\n","      model.finetune(\n","          mixture_or_task_name=task_to_train,\n","          pretrained_model_dir=PRETRAINED_DIR,\n","          finetune_steps=number_of_steps\n","      )"],"execution_count":6,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YAKKKClFdAZY"},"source":["# Evaluation\n","\n","Evaluate the model checkpoint(s) on the validation set"]},{"cell_type":"code","metadata":{"id":"HOM1HIQbln3a","executionInfo":{"status":"ok","timestamp":1630591380567,"user_tz":-120,"elapsed":194,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["# Use a larger batch size for evaluation, which requires less memory.\n","model.batch_size = 1024\n","model.eval(\n","    mixture_or_task_name=task_to_train,\n","    # -1 will evaluate the last checkpoint, you can also provide \n","    # a list of checkpoints with the following format : [10000, 20000, 30000]\n","    checkpoint_steps=-1,\n","    split=\"validation\"\n","    )"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ur8eEd8dQCTX"},"source":["# Confidence Score\n","\n","Using the `model.score()` function we evaluate the model confidence about the generated predictions (given the input).\n","\n","NOTE: To generate the predictions follow the instructions in our  [replication package](https://github.com/CodeReviewAutomation/code_review_automation)"]},{"cell_type":"code","metadata":{"id":"m9m79JaPPB_-","executionInfo":{"status":"ok","timestamp":1630591390244,"user_tz":-120,"elapsed":168,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["import math\n","\n","# upload your input file (source.txt) and your prediction file (predictions.txt)\n","\n","inputs_file = './source.txt'\n","pred_file = 'predictions.txt'\n","score_file = 'score'\n","model.score(inputs=inputs_file,\n","            targets=pred_file,\n","            scores_file=score_file,\n","            checkpoint_steps='best',\n","            vocabulary=get_default_vocabulary())\n","\n","confidence_score = [math.exp(float(line.split)) for line in open('./score.score', 'r')]\n","\n","f = open('./confidence_score.txt', 'w+')\n","for i in range(len(confidence_score)):\n","  f.write(str(confidence_score[i]) + '\\n')\n","f.close()"],"execution_count":8,"outputs":[]}]}