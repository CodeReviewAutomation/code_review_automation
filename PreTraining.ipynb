{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PreTraining.ipynb","provenance":[{"file_id":"https://github.com/masies/CRA/blob/main/replication_package/Replication_package_PreTraining.ipynb","timestamp":1630436132400}],"collapsed_sections":[],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"TPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"3eL3d9EP2KVW"},"source":["# T5 Pre-Training\n","\n","in this notebook we will pre-train a T5 small model on the dataset we already processed.\n","\n","## NOTEBOOK SETTINGS\n","\n","We recommend to use \"high ram\" setting for this notebook\n","you can changed this in the colab menu : `Runtime > Change runtime type`\n","\n","\n","We start by setting the environment connecting colab to the Google Cloud Storage (GCS) bucket and setting everything up for the TPU processor. (This colab uses TPU and high ram settings)"]},{"cell_type":"code","metadata":{"id":"97N9NUCyrJyk","executionInfo":{"status":"ok","timestamp":1630588749144,"user_tz":-120,"elapsed":3,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["from google.colab import auth\n","auth.authenticate_user()\n","#@title ## Set Your GCS credential\n","project_id = '' #@param {type:\"string\"}\n","bucket_name = '' #@param {type:\"string\"}\n","\n","!gcloud config set project {project_id}\n","\n","!pip3 install --upgrade pip\n","!pip3 install t5==0.9.1\n","!pip3 install tensorflow==2.6.0\n","!pip3 install keras==2.6.0\n","!pip3 install gin-config\n","\n","import functools\n","import os\n","import time\n","import warnings\n","warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n","import tensorflow.compat.v1 as tf\n","tf.enable_eager_execution()\n","import tensorflow_datasets as tfds\n","import t5\n","\n","#Set the base dir(Google cloud bucket)\n","BASE_DIR = \"gs://\"+bucket_name\n","\n","if not BASE_DIR or BASE_DIR == \"gs://\":\n","  raise ValueError(\"You must enter a BASE_DIR.\")\n","ON_CLOUD = True\n","\n","if ON_CLOUD:\n","  import tensorflow_gcs_config\n","  from google.colab import auth\n","  # Set credentials for GCS reading/writing from Colab and TPU.\n","  TPU_TOPOLOGY = \"2x2\"\n","  try:\n","    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n","    TPU_ADDRESS = tpu.get_master()\n","    print('Running on TPU:', TPU_ADDRESS)\n","  except ValueError:\n","    raise BaseException('ERROR: Not connected to a TPU runtime; please see the previous cell in this notebook for instructions!')\n","  auth.authenticate_user()\n","  tf.config.experimental_connect_to_host(TPU_ADDRESS)\n","  tensorflow_gcs_config.configure_gcs_from_colab_auth()\n","\n","tf.disable_v2_behavior()\n","\n","# Improve logging.\n","from contextlib import contextmanager\n","import logging as py_logging\n","\n","if ON_CLOUD:\n","  tf.get_logger().propagate = False\n","  py_logging.root.setLevel('INFO')\n","\n","@contextmanager\n","def tf_verbosity_level(level):\n","  og_level = tf.logging.get_verbosity()\n","  tf.logging.set_verbosity(level)\n","  yield\n","  tf.logging.set_verbosity(og_level)"],"execution_count":1,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r80Et9u07UqI"},"source":["We specify the path of our masked pre-training dataset (the tsv file) in the GCS bucket "]},{"cell_type":"code","metadata":{"id":"71L-PfW1rTWp"},"source":["masked_pretraining_dataset_path = \"gs://\" + bucket_name + \"/automating_code_review/dataset/pre-training/pre-training.tsv\"\n","\n","nq_tsv_path = {\n","    \"train\": masked_pretraining_dataset_path\n","}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LzPfEvjG7h4A"},"source":["We specify the model and vocab path of the previusly trained sentencepiece tokenizer model in the GCS bucket"]},{"cell_type":"code","metadata":{"id":"ZlG5DWwXrTZH"},"source":["from t5.data import postprocessors as t5_postprocessors\n","from t5.seqio import Feature,SentencePieceVocabulary\n","\n","vocab_model_path = \"gs://\" + bucket_name + \"/automating_code_review/tokenizer/TokenizerModel.model\"\n","vocab_path = \"gs://\" + bucket_name + \"/automating_code_review/tokenizer/TokenizerModel.vocab\"\n","\n","TaskRegistry = t5.data.TaskRegistry\n","TfdsTask = t5.data.TfdsTask\n","\n","def get_default_vocabulary():\n","  return SentencePieceVocabulary(vocab_model_path, 100)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NIaqnmWQ7x0-"},"source":["We scan our dataset to generate input/output pairs"]},{"cell_type":"code","metadata":{"id":"GqVviL-p7x92","executionInfo":{"status":"ok","timestamp":1630588763540,"user_tz":-120,"elapsed":204,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["DEFAULT_OUTPUT_FEATURES = {\n","    \"inputs\": Feature(\n","        vocabulary=get_default_vocabulary(), add_eos=False, required=True),\n","\n","    \"targets\": Feature(\n","        vocabulary=get_default_vocabulary(), add_eos=False)\n","}\n","\n","def nq_dataset_fn(split, shuffle_files=True):\n","  # We only have one file for each split.\n","  del shuffle_files\n","\n","  # Load lines from the text file as examples.\n","  ds = tf.data.TextLineDataset(nq_tsv_path[split])\n","  ds = ds.map(\n","      functools.partial(tf.io.decode_csv, record_defaults=[\"string\",\"string\"],\n","                        field_delim=\"\\t\", use_quote_delim=False),\n","      num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","  ds = ds.map(lambda *ex: dict(zip([\"input\", \"output\"], ex)))\n","  return ds\n","\n","# print(\"A few raw train examples...\")\n","for ex in tfds.as_numpy(nq_dataset_fn(\"train\").take(3)):\n","  print(ex)"],"execution_count":2,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JDV2BJCq8Hwa"},"source":["We create a task for training T5, here we specify the input & output maximum sizes : 512 tokens."]},{"cell_type":"code","metadata":{"id":"KV1iaXO2rTeQ","executionInfo":{"status":"ok","timestamp":1630588770237,"user_tz":-120,"elapsed":188,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["def preprocessing(ds):\n","  def to_inputs_and_targets(ex):\n","        inputs = tf.strings.join([ ex['input']], separator=' ')\n","        class_label = tf.strings.join([ex['output']], separator=' ')\n","        return {'inputs': inputs, 'targets': class_label }\n","  return ds.map(to_inputs_and_targets, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","\n","#Create a new training task\n","t5.data.TaskRegistry.remove('pretraining')\n","t5.data.TaskRegistry.add(\n","    \"pretraining\",\n","    t5.data.Task,\n","    dataset_fn=nq_dataset_fn,\n","    splits=[\"train\", \"validation\"],\n","    text_preprocessor=[preprocessing],\n","    output_features = DEFAULT_OUTPUT_FEATURES,\n","    metric_fns=[t5.evaluation.metrics.accuracy],\n",")\n","\n","nq_task = t5.data.TaskRegistry.get(\"pretraining\")\n","ds = nq_task.get_dataset(split=\"train\", sequence_length={\"inputs\": 512, \"targets\": 512})\n","print(\"A  preprocessed training example...\")\n","for ex in tfds.as_numpy(ds.take(1)):\n","  print(ex)"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dvcbcscy8U5f"},"source":["We set up the model size (small), the model batch size (256), and the path to save the checkpoints.\n","checkpoints will be saved each 10000 steps."]},{"cell_type":"code","metadata":{"id":"2LE3b21SrTmA"},"source":["from mesh_tensorflow.transformer.learning_rate_schedules import learning_rate_schedule_noam\n","\n","MODEL_SIZE = \"small\"  \n","\n","MODEL_DIR = \"gs://\" + bucket_name + \"/automating_code_review/model_dumps/pre-training/\"\n","\n","model_parallelism, train_batch_size, keep_checkpoint_max = {\n","    \"small\": (1, 256, 16),\n","    \"base\": (2, 128, 8),\n","    \"large\": (8, 64, 4),\n","    \"3B\": (8, 16, 1),\n","    \"11B\": (8, 16, 1)}[MODEL_SIZE]\n","\n","tf.io.gfile.makedirs(MODEL_DIR)\n","\n","model = t5.models.MtfModel(\n","    model_dir=MODEL_DIR,\n","    tpu=TPU_ADDRESS,\n","    tpu_topology=TPU_TOPOLOGY,\n","    model_parallelism=model_parallelism,\n","    batch_size=train_batch_size,\n","    sequence_length={\"inputs\": 512, \"targets\": 512},\n","    learning_rate_schedule = learning_rate_schedule_noam,\n","    save_checkpoints_steps=10000,\n","    keep_checkpoint_max=keep_checkpoint_max if ON_CLOUD else None\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUhmuebe8nzD"},"source":["Finally, we run the pre-training for 200000 steps"]},{"cell_type":"code","metadata":{"id":"iIynIx4Irgd1","executionInfo":{"status":"ok","timestamp":1630588779731,"user_tz":-120,"elapsed":178,"user":{"displayName":"Gatto Obeso","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiVUtCE48nIGGOZXqMjPc7z02bo2jmQ46tTvkbWJfw=s64","userId":"16033737261253901932"}}},"source":["# We used 200000 TRAIN_STEPS\n","PATH_GIN_FILE = \"gs://\" + bucket_name + \"/automating_code_review/model_dumps/pre-training/operative_config.gin\"\n","import gin\n","with gin.unlock_config():    \n","    gin.parse_config_file(PATH_GIN_FILE)\n","    TRAIN_STEPS = 200000\n","    model.train(\"pretraining\", steps=TRAIN_STEPS)"],"execution_count":4,"outputs":[]}]}